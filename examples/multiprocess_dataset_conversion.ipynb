{
    "cells": [
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "daY0p3RNvzFR"
            },
            "source": [
                "# Multiprocess dataset conversion\n",
                "\n",
                "If your dataset is huge, running single process dataset conversion script could be very time consuming. You can use multiprocessing with MDSWriter to convert your dataset in parallel. There are few ways in which you can convert your raw data into MDS format parallelly.\n",
                "\n",
                "1. Download a raw data in parallel and convert to MDS format sequentially.\n",
                "2. Group a raw data and convert to MDS format parallely in separate sub-directories and then merge all the sub-directories index.json file to get a unified MDS dataset.\n",
                "\n",
                "Let's look at the small example of each one on how to that."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "VKTtgJjkvzFU"
            },
            "source": [
                "## 1. Fetch raw data in parallel and write sequentially\n",
                "For a large individual dataset file such as image or a video, it would be useful to download those files in parallel by multiple processes and once it is downloaded, call the MDSWriter to write the data into MDS format. Below is one such example on how to do that."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "xq9Icki3uRRu"
            },
            "source": [
                "### Setup\n",
                "\n",
                "Let's start by making sure the right packages are installed and imported. We need to install the `mosaicml-streaming` package which installs the sufficient dependencies to run this tutorial."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Wq_oqcwrv08e"
            },
            "outputs": [],
            "source": [
                "%pip install mosaicml-streaming"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "T-9o1UQivzFV"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "from multiprocessing import Pool\n",
                "\n",
                "from streaming import MDSWriter, StreamingDataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "gjz0OwY9u04a"
            },
            "source": [
                "### Global settings\n",
                "\n",
                "Initialize the global variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "6HDCKMaTu8U3"
            },
            "outputs": [],
            "source": [
                "out_root = './data'\n",
                "# This could be a list of URLs needs to download\n",
                "dataset = [i for i in range(25)]\n",
                "columns = {'number': 'int'}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "tq4NInVovzFW"
            },
            "source": [
                "Download data from URL. Here, we just return a number for demonstration purpose. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-NonDeBovzFW"
            },
            "outputs": [],
            "source": [
                "def get_data(number):\n",
                "    print(f'\\nWorker PID: {os.getpid()}\\tnumber: {number}', flush=True, end='')\n",
                "    # Add code here to downloads the data from URL.\n",
                "    return {'number': number}"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "yByl7cpsvzFX"
            },
            "source": [
                "Initialization method for each worker process which prints the worker PID."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "Ut8clRDvvzFY"
            },
            "outputs": [],
            "source": [
                "# Initialize the worker process\n",
                "def init_worker():\n",
                "    # Get the pid for the current worker process\n",
                "    pid = os.getpid()\n",
                "    print(f'\\nInitialize Worker PID: {pid}', flush=True, end='')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "cjtN8ZEwvzFY"
            },
            "source": [
                "### Convert to MDS format\n",
                "\n",
                "Initialize 4 worker processes which downloads the data in parallel and once the data is ready, it is getting written in MDS format using `write` method call."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "xV-SRpZkvzFY"
            },
            "outputs": [],
            "source": [
                "# clean up root directory\n",
                "%rm -rf $out_root\n",
                "\n",
                "with Pool(initializer=init_worker, processes=4) as pool:\n",
                "    with MDSWriter(out=out_root, columns=columns) as out:\n",
                "        for sample in pool.imap(get_data, dataset):\n",
                "            out.write(sample)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "5cWmR7svvzFZ"
            },
            "source": [
                "### Load MDS dataset\n",
                "\n",
                "Read the sample using `StreamingDataset` which prints the sample ID."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "e-iQeOecvzFZ"
            },
            "outputs": [],
            "source": [
                "# read the sample\n",
                "dataset = StreamingDataset(local=out_root,\n",
                "                           remote=None,\n",
                "                           shuffle=False,)\n",
                "for sample in dataset:\n",
                "    print(sample['number'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "WXF-OdO0wYb8"
            },
            "outputs": [],
            "source": [
                "# Clean up\n",
                "%rm -rf $out_root"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "WXybtE1e5Nzo"
            },
            "source": [
                "## 2. Group the raw data and convert to MDS format in parallel\n",
                "\n",
                "For a large dataset file such as a tar file, zip file, or any other file, we would recommend to map one raw data file to one MDS sub-directories so that the dataset conversion happens by multiple process in parallel."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "3GbXadoPJne7"
            },
            "source": [
                "Import dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "NcfrL7ynJmyF"
            },
            "outputs": [],
            "source": [
                "import os\n",
                "import json\n",
                "from glob import glob\n",
                "from typing import Iterator, Tuple\n",
                "\n",
                "from multiprocessing import Pool\n",
                "\n",
                "from streaming import MDSWriter, StreamingDataset"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "45Xz1lLSUnug"
            },
            "source": [
                "### Global settings\n",
                "\n",
                "Initialize the global variable"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "q8AHVCVoUoU5"
            },
            "outputs": [],
            "source": [
                "out_root = './group_data'\n",
                "num_groups = 4\n",
                "num_process = 2"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "1qkyMzckKl0V"
            },
            "source": [
                "Get a sub-directory MDS path and raw dataset sample range of 10. For example, first sub-directory yields a sample from 0 to 9, second sub-directory yields a sample from 10 to 19, and so on.\n",
                "\n",
                "If you are working with a large file, you can also yield a raw dataset file path instead of sample range."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "PZWRXNKEKkSm"
            },
            "outputs": [],
            "source": [
                "def each_task(out_root: str, groups: int) -> Iterator[Tuple[str, int, int]]:\n",
                "    \"\"\"Get the sub-directory path and the sample range.\n",
                "\n",
                "    Args:\n",
                "        out_root (str): base output mds directory\n",
                "        groups (int): Number of sub-directories to create\n",
                "\n",
                "    Yields:\n",
                "        Iterator[Tuple[str, int, int]]: Each argument tuple\n",
                "    \"\"\"\n",
                "    for data_group in range(groups):\n",
                "        sub_out_root = os.path.join(out_root, str(data_group))\n",
                "        start_sample_idx = data_group * 10\n",
                "        end_sample_idx = start_sample_idx + 9\n",
                "        yield sub_out_root, start_sample_idx, end_sample_idx"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "p9XqWLD-Moqz"
            },
            "source": [
                "Convert a raw dataset into MDS format. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "h6zB2yLVMpIb"
            },
            "outputs": [],
            "source": [
                "def convert_to_mds(args: Iterator[Tuple[str, int, int]]) -> None:\n",
                "    \"\"\"Convert raw dataset into MDS format\n",
                "\n",
                "    Args:\n",
                "        args (Iterator[Tuple[str, int, int]]): All arguments, packed into a tuple because\n",
                "            process pools only pass one argument.\n",
                "\n",
                "    Yields:\n",
                "        Dict: A sample\n",
                "    \"\"\"\n",
                "    sub_out_root, start_sample_idx, end_sample_idx = args\n",
                "\n",
                "    def get_data(start: int, end: int):\n",
                "        for i in range(start, end + 1):\n",
                "            yield {'number': i}\n",
                "    \n",
                "    columns = {'number': 'int'}\n",
                "\n",
                "    with MDSWriter(out=sub_out_root,\n",
                "                   columns=columns) as out:\n",
                "        for sample in get_data(start_sample_idx, end_sample_idx):\n",
                "            out.write(sample)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "by7aPVIDM1mG"
            },
            "source": [
                "Divide the dataset into 4 sub-groups, each process takes a sub-group and converts a data into MDS format in their respective sub-directories."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cZiFudkCM2dF"
            },
            "outputs": [],
            "source": [
                "# clean up root directory\n",
                "%rm -rf $out_root\n",
                "\n",
                "arg_tuples = each_task(out_root, groups=num_groups)\n",
                "    \n",
                "# Process group of data in parallel into directories of shards.\n",
                "with Pool(initializer=init_worker, processes=num_process) as pool:\n",
                "    for count in pool.imap(convert_to_mds, arg_tuples):\n",
                "        pass\n",
                "print('Finished')"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "dX60JcG9M_aT"
            },
            "source": [
                "Once dataset has been converted to an MDS format, let's look at the directory structure. You will find 4 sub-directories and each sub-directories contain a `index.json` file and a shard files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "_Ty7i0WwNCEa"
            },
            "outputs": [],
            "source": [
                "%ll $out_root"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "Ccye5lJnNFSm"
            },
            "source": [
                "### Merge meta data\n",
                "\n",
                "The last step of the conversion process is to merge all the sub-directories `index.json` file. The content of the Shard files will remain as it is. By calling the merge_index utility function, the global shard information will be written to a new `index.json` file placed in `out`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "lz3Aem5WNQ2R"
            },
            "outputs": [],
            "source": [
                "from streaming.base.util import merge_index\n",
                "merge_index(out_root, keep_local=True)"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "NByc1ZGINcXe"
            },
            "source": [
                "Let's checkout the root directories where you can see one `index.json` file and many shard files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "hrD9yGgPNclL"
            },
            "outputs": [],
            "source": [
                "%ll $out_root"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "XZ1-fIIMNVO8"
            },
            "source": [
                "### Load MDS dataset\n",
                "\n",
                "Read the sample using `StreamingDataset` which prints the sample ID."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "FlRod4cnNVlM"
            },
            "outputs": [],
            "source": [
                "# read the sample\n",
                "dataset = StreamingDataset(local=out_root,\n",
                "                           remote=None,\n",
                "                           shuffle=False)\n",
                "for ix, sample in enumerate(dataset):\n",
                "    print(sample['number'])"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "AKRdDahKWyki"
            },
            "source": [
                "### Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "_esj_q2MNiny"
            },
            "outputs": [],
            "source": [
                "%rm -rf $out_root"
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "lf00woabVLWG"
            },
            "source": [
                "\n",
                "## What next?\n",
                "\n",
                "You've now seen an in-depth tutorial on converting a dataset into MDS format using multiple process. If you are interested in the real world example, then, checkout the [WebVid](https://github.com/mosaicml/streaming/blob/main/streaming/multimodal/convert/webvid/crawl_webvid.py) and [Pile](https://github.com/mosaicml/streaming/blob/main/streaming/text/convert/pile.py) dataset conversion scripts which converts the dataset into MDS format via multiprocessing."
            ]
        },
        {
            "attachments": {},
            "cell_type": "markdown",
            "metadata": {
                "id": "dvL3gQ3OXMZo"
            },
            "source": [
                "## Come get involved with MosaicML!\n",
                "\n",
                "We'd love for you to get involved with the MosaicML community in any of these ways:\n",
                "\n",
                "### [Star Streaming on GitHub](https://github.com/mosaicml/streaming)\n",
                "\n",
                "Help make others aware of our work by [starring Streaming on GitHub](https://github.com/mosaicml/streaming).\n",
                "\n",
                "### [Join the MosaicML Slack](https://mosaicml.me/slack)\n",
                "\n",
                "Head on over to the [MosaicML slack](https://mosaicml.me/slack) to join other ML efficiency enthusiasts. Come for the paper discussions, stay for the memes!\n",
                "\n",
                "### Contribute to Streaming\n",
                "\n",
                "Is there a bug you noticed or a feature you'd like? File an [issue](https://github.com/mosaicml/streaming/issues) or make a [pull request](https://github.com/mosaicml/streaming/pulls)!"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
